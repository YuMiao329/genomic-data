---
title: "breast_cancer"
author: "Patrick Garr"
date: '2022-04-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(corrplot)
library(car)
library(lmtest)
library(MLmetrics)
library(glmnet)
library(caTools)
library(splines)
library(psych)
library(mgcv)
library(graphics)
library(relaimpo) # Package for calculating relative importance metrics
library(gridExtra)
library(factoextra)
library(ROCR)
library(MASS)
library(gmodels)
library(class)
library(e1071) # Used for SVM model
library(mlbench) # Used for LVQ model

set.seed(1000)
```

```{r}
# Wisconsin diagnostic breast cancer dataset
wdbc.data <- read.csv("wdbc.csv", sep = ",",
         header = TRUE, stringsAsFactors = TRUE)

# Removing the id attribute as it's not needed for this analysis. Also removing unknown X variable with no data in it
wdbc.data <- wdbc.data[,-1]
wdbc.data <- wdbc.data[,-32]

nrows.LE <- nrow(wdbc.data)
ncol.LE <- length(wdbc.data)

print(paste("There are",nrows.LE, "rows and",ncol.LE, "columns in the breast cancer dataset"))

summary(wdbc.data)

```


```{r echo=FALSE}
# Creating a box plot examining the ten factors' means with diagnosis outcomes to get a simple graphical view
boxplot1 <- ggplot(wdbc.data, aes(radius_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Radius vs. Diagnosis', x = 'Radius', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot2 <- ggplot(wdbc.data, aes(texture_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Texture vs. Diagnosis', x = 'Texture', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot3 <- ggplot(wdbc.data, aes(perimeter_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Perimeter vs. Diagnosis', x = 'Perimeter', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot4 <- ggplot(wdbc.data, aes(area_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Area vs. Diagnosis', x = 'Area', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot5 <- ggplot(wdbc.data, aes(smoothness_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Smoothness vs. Diagnosis', x = 'Smoothness', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 5))

boxplot6 <- ggplot(wdbc.data, aes(compactness_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Compactness vs. Diagnosis', x = 'Compactness', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot7 <- ggplot(wdbc.data, aes(concavity_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Concavity vs. Diagnosis', x = 'Concavity', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot8 <- ggplot(wdbc.data, aes(concave.points_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Concave Points vs. Diagnosis', x = 'Concave Points', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot9 <- ggplot(wdbc.data, aes(symmetry_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Symmetry vs. Diagnosis', x = 'Symmetry', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot10 <- ggplot(wdbc.data, aes(fractal_dimension_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  theme_grey(base_size = 8)+
  coord_flip()+
  labs(title='Fractal Dimension vs. Diagnosis', x = 'Fractal Dimension', y = 'Diagnosis')+
  theme(plot.title = element_text(hjust = 0.5, size = 5))

grid.arrange(boxplot1,boxplot2,boxplot3,boxplot4,boxplot5,boxplot6,boxplot7,boxplot8,boxplot9,boxplot10, ncol=5, nrow =2, top = textGrob("Boxplots of Cell Nucleus Features (Mean Value)",gp=gpar(fontsize=15,font=3)))

```

```{r}
# Creating a density plot examining the ten factors' means
pt1 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x =radius_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = radius_mean, y=..density..)) +
    labs(title='density function of radius_mean', x='radius_mean', y='density') 
pt2 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = texture_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = texture_mean, y=..density..)) +
    labs(title='density function of texture_mean', x='texture_mean', y='density')
pt3 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = perimeter_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = perimeter_mean, y=..density..)) +
    labs(title='density function of perimeter_mean', x='perimeter_mean', y='density')
pt4 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = area_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = area_mean, y=..density..)) +
    labs(title='density function of area_mean', x='area_mean', y='density')
pt5 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = smoothness_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = smoothness_mean, y=..density..)) +
    labs(title='density function of smoothness_mean', x='smoothness_mean', y='density')
pt6 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = compactness_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = compactness_mean, y=..density..)) +
    labs(title='density function of compactness_mean', x='compactness_mean', y='density')
pt7 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = concavity_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = concavity_mean, y=..density..)) +
    labs(title='density function of concavity_mean', x='concavity_mean', y='density')
pt8 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = concave.points_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = concave.points_mean, y=..density..)) +
    labs(title='density function of concave.points_mean', x='concave.points_mean', y='density')
pt9 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = symmetry_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = symmetry_mean, y=..density..)) +
    labs(title='density function of symmetry_mean', x='symmetry_mean', y='density')
pt10 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = fractal_dimension_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = fractal_dimension_mean, y=..density..)) +
    labs(title='density function of fractal_dimension_mean', x='fractal_dimension_mean', y='density')
grid.arrange(pt1,pt2,pt3,pt4,pt5,pt6,pt7,pt8,pt9,pt10, ncol=4, nrow =3)
```

```{r}

corrplot(cor(wdbc.data[,2:31]), method = 'circle', title = "Correlation Plot of Breast Cancer Dataset Variables", mar=c(0,0,1,0))
# wdbc.matrix <- as.matrix(wdbc.data[,2:31])
# heatmap(wdbc.matrix)

```



```{r}
# Changing the benign and malignant diagnoses to 1 and 0, respectively
wdbc.data$diagnosis <- as.character(wdbc.data$diagnosis)
wdbc.data$diagnosis <- replace(wdbc.data$diagnosis, wdbc.data$diagnosis == "B","1")
wdbc.data$diagnosis <- replace(wdbc.data$diagnosis, wdbc.data$diagnosis == "M","0")
wdbc.data$diagnosis <- as.factor(wdbc.data$diagnosis)

# Scaling and indexing data to create training and testing sets. Using 70% split
wdbc.df <- wdbc.data
wdbc.df[2:31] <- scale(wdbc.df[2:31])

idx <- sample(nrow(wdbc.df),nrow(wdbc.df)*0.7)

train.df <- as.data.frame((wdbc.df[idx,]))
test.df1 <- as.data.frame(wdbc.df[-idx,])
test.df <- test.df1[,-c(1)]

train.labels <- train.df[,1]
test.labels <- test.df1[,1]

```


```{r}
# Performing PCA to see if we can narrow down the features
pr.wdbc <- prcomp(wdbc.df[2:31], scale=FALSE)
PCA.eig <- get_eig(pr.wdbc)
# 3 PCs explain 73% of the data, 5 PCs explain 85%, and 10 PCs explain 95%

# Looking at the top variables in the top 3 PCs
wdbc.load <- pr.wdbc$rotation
PC1.top10 <- sort(abs(wdbc.load[,1]),decreasing = TRUE)[1:10]
PC2.top10 <- sort(abs(wdbc.load[,2]),decreasing = TRUE)[1:10]
PC3.top10 <- sort(abs(wdbc.load[,3]),decreasing = TRUE)[1:10]

fviz_eig(pr.wdbc, addlabels = TRUE)
# Significant elbow seen after the 2nd or 3rd PC

fviz_pca_biplot(pr.wdbc, label="var", repel = TRUE) +
   labs(title ="PCA Biplot of All Features", x = paste("PC1 -", round(PCA.eig$variance.percent[1],1),"%"), y = paste("PC2 -", round(PCA.eig$variance.percent[2],1),"%"))

```

```{r}
# Feature selection with R caret using the Learning Vector Quantization method
# Prepare the training scheme with repeated cross-validation method
control <- trainControl(method="repeatedcv", number=10, repeats=3)
lvq.wdbc <- train(diagnosis~., data=wdbc.df, method="lvq", trControl=control)

# Use the model to estimate relative importances
importance <- varImp(lvq.wdbc, scale=FALSE)
plot(importance, main = "Variable Importances", ylab = "Features")


```

```{r}
# k-means clustering

# Determining the optimal number of clusters using two methods
fviz_nbclust(wdbc.df[2:31], kmeans, method ="wss") +
  labs(subtitle = "Wss Method")
fviz_nbclust(wdbc.df[2:31], kmeans, method ="silhouette")+
  labs(subtitle = "Silhouette Method")
# Both methods indicate that 2 clusters is enough 

kmeans.wdbc = kmeans(wdbc.df[2:31], center = 2)
# kmeans_df3

fviz_cluster(kmeans.wdbc, wdbc.df[2:31])+
  labs(subtitle = "k-means Model of Breast Cancer Data with 2 Clusters")

kmeans.pred <- as.factor(kmeans.wdbc$cluster-1)
confmat.kmeans <- confusionMatrix(kmeans.pred,wdbc.df[,1])
confmat.kmeans

```


```{r}
# Hierarchical clustering: Euclidian method
dist.wdbc1 = dist(wdbc.df[2:31], method = "euclidean")
fviz_dist(dist.wdbc1)+
  labs(title = "Euclidian Dissimilarity Matrix")

hcl.wdbc1 <- hclust(dist.wdbc1, method = 'complete')
plot(hcl.wdbc1, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc1, k = 2, border = 'red')

cutHcl1 = cutree(hcl.wdbc1, k = 2)
clusterTab1 = table(cutHcl1, wdbc.df[,1])
clusterTab1

```


```{r}
# Hierarchical clustering: Euclidian method w/ larger k

# Finding optimal number of ks using a for loop, commenting it after running
# range <- c(1:10)
# 
# for (val in range) { 
#   cutHcl = cutree(hcl.wdbc, k =val)
#   clusterTab = table(cutHcl, wdbc.df[,1])
#   print(clusterTab)
#   print(paste0(val, " Clusters"))}

# Optimal clusters seems to be at 4 as that is where the largest difference in clusters occurs

dist.wdbc2 = dist(wdbc.df[2:31], method = "euclidean")

hcl.wdbc2 <- hclust(dist.wdbc2, method = 'complete')
plot(hcl.wdbc2, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc2, k = 4, border = 'red')

cutHcl2 = cutree(hcl.wdbc2, k = 4)
clusterTab2 = table(cutHcl2, wdbc.df[,1])
clusterTab2

```

```{r}
# Hierarchical clustering: Manhattan method

dist.wdbc3 = dist(wdbc.df[2:31], method = "manhattan")
hcl.wdbc3 <- hclust(dist.wdbc3, method = 'ward.D')

plot(hcl.wdbc3, cex = 0.6, hang = -1,
     labels = FALSE,
     main = "Hierarchical Clustering Dendrogram",
     sub = "(Using the Manhattan Method)",
     cex.sub = 0.75,
     xlab = "Clusters")
rect.hclust(hcl.wdbc3, k = 2, border = 'red')


cutHcl3 = cutree(hcl.wdbc3, k = 2)
cutHcl3 <- as.factor(cutHcl3-1)
# clusterTab3 = table(cutHcl3, wdbc.df[,1])
# clusterTab3
confmat.hcl <- confusionMatrix(cutHcl3,wdbc.df[,1])
confmat.hcl

```

```{r}
# Write a function to calculcate the misclassification rate
cal_misclassification <- function(confusionTab){
  misc = (confusionTab[1,2]+confusionTab[2,1])/sum(confusionTab)
  print(paste("misclassification rate: ", misc))
}
```



```{r}
set.seed(1)
# use unscaled data
train.tr <- as.data.frame((wdbc.data[idx,]))
test.tr1 <- as.data.frame(wdbc.data[-idx,])
test.tr <- test.tr1[,-c(1)]
train.labels <- train.df[,1]
test.labels.tr <- test.tr1[,1]
library(tree)
# Use a decision tree to find which features are more significant
wdbc.tree = tree(diagnosis~., data =train.tr)
summary(wdbc.tree)
# Make a plot of the tree to visulize
plot(wdbc.tree)
text(wdbc.tree, pretty = 0)
# One of the problems, however, is that they tend to overfit when used 'out of the box'
treePred = predict(wdbc.tree, test.tr, type='class')
confusionTab = table(Predicted = treePred, Actual = test.labels.tr)
confusionTab
cal_misclassification(confusionTab)
confusionMatrix(treePred,test.labels.tr)
```
According to the confusion matrix, we could see that the misclassification rate is very low.


```{r}
# Although the model already has very good performance, we Use cross validation to see if the performance could be further improved, or if we could reach similar performance use a more simple model.
set.seed(1)
cv.wdbc.tree = cv.tree(wdbc.tree, FUN = prune.misclass)
plot(cv.wdbc.tree, main = ("Cross-validated decision tree"))
# The lowest misclassification and simplest model is node= 4.
prune.wdbc = prune.misclass(wdbc.tree, best = 4)
print(plot(prune.wdbc),top=textGrob("Prune of cross-validated decision tree"))
text(prune.wdbc, pretty = 0)
treePred2 = predict(prune.wdbc, test.tr, type='class')
# confusionTab2 = table(Predicted = treePred2, Actual = test.labels.tr)
# confusionTab2
# cal_misclassification(confusionTab2)
confmat.rand <- confusionMatrix(treePred2, test.labels.tr)
confmat.rand
```
We could see that the model has smaller size with same misclassification rate now.


```{r}
# mtry is the number of variables randomly sampled at each split. Use sqrt(p) here for classification
library(randomForest)
wdbc.rf = randomForest(diagnosis~., data = train.tr, mtry = sqrt(ncol(train.tr)), ntree = 500)
wdbc.rf
rfPreds = predict(wdbc.rf, test.tr)
plot(wdbc.rf, main = ("Random forest"))
summary(wdbc.rf)
importance(wdbc.rf)
varImpPlot(wdbc.rf)
bestmtry <- tuneRF(train.tr[2:31], train.tr$diagnosis, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)
# Use the best mtry
optRf = randomForest(diagnosis~., data = train.tr, mtry = 7, ntree = 320)
plot(optRf, main = ("Optimal Random forest"))
importance(optRf)
varImpPlot(optRf)
pred.rf = predict(optRf, test.tr)
confusionMatrix(pred.rf, test.labels.tr)
```



```{r}
# LDA

lda.bc = lda(diagnosis ~., data = train.df)

pred.lda1 = predict(lda.bc, train.df)
pred.lda2 = predict(lda.bc, test.df)

confmat.lda1 <- confusionMatrix(pred.lda1$class, train.labels)
confmat.lda2 <- confusionMatrix(pred.lda2$class, test.labels)

confmat.lda1
confmat.lda2

# Making ROC curves for better visualization of the data
ROC.df.lda <- data.frame(pred.lda2$posterior[,2])
ROC.df.lda$labels <- test.labels

ROC.pred.lda <- prediction(ROC.df.lda[1],ROC.df.lda[2])
ROC.perf.lda <- performance(ROC.pred.lda, "tpr","fpr")
plot(ROC.perf.lda, main = "ROC Curve of LDA Model")

auc.lda <- performance(ROC.pred.lda, measure = 'auc')
auc.lda <- auc.lda@y.values
print(paste("The AUC of this curve is:", auc.lda))

```


```{r}
# Logistic regression

wdbc.log = glm(diagnosis ~., family = binomial, data = train.df)
log.fit.cont = predict(wdbc.log, newdata = test.df, type = 'response')
log.fit = ifelse(log.fit.cont > 0.5, '1', '0')

log.fit.train = predict(wdbc.log, newdata = train.df, type = 'response')
log.fit.train = ifelse(log.fit.train > 0.5, '1', '0')

pred.log.train = as.factor(log.fit.train)
pred.log.test = as.factor(log.fit)

confmat.log1 <- confusionMatrix(pred.log.train, train.labels)
confmat.log2 <- confusionMatrix(pred.log.test, test.labels)

confmat.log1
confmat.log2

# Making ROC curves for better visualization of the data
ROC.log <- data.frame(log.fit.cont)
ROC.log$labels <- test.labels

ROC.pred.log <- prediction(ROC.log[1],ROC.log[2])
ROC.perf.log <- performance(ROC.pred.log, "tpr","fpr")
plot(ROC.perf.log, main = "ROC Curve of Logistic Regression Model")

auc.log <- performance(ROC.pred.log, measure = 'auc')
auc.log <- auc.log@y.values
print(paste("The AUC of this curve is:", auc.log))

```

```{r}
# knn model

# Determining best k using a for loop to create multiple models and compare their accuracy values
# Initialize the dataframe
acc.df <- as.data.frame(matrix(nrow=1,ncol=1))
colnames(acc.df) <- c('knn value')

for (i in 1:20) {
  knn.wdbc <- knn(train.df,test.df1,train.labels, k = i)
  knn.conf.mat <- confusionMatrix(knn.wdbc,test.labels)
  acc.df$Accuracy <- knn.conf.mat$overall[1] # Adding the accuracy to a new row
  new <- rep(knn.conf.mat$overall[1], ncol(acc.df))                       
  acc.df[nrow(acc.df) + 1, ] <- new
}

# Cleaning up the dataframe
colnames(acc.df) <- c('Accuracy', 'KNN Value')
acc.df <- acc.df[-c(1),]
acc.df$`KNN Value` <- c(1:20)
acc.df<- acc.df[order(-acc.df$Accuracy),] # Sorting in descending order to see
head(acc.df)                              # best value for model

# Multiple values give the same accuracy so choosing 10 for the model to keep it flexible for other datasets if needed

knn.wdbc.best <- knn(train.df,test.df1,train.labels, k = 10)
confmat.knn <- confusionMatrix(knn.wdbc.best,test.labels)
confmat.knn

# Repeating above, but this time repeating with CV training control
control <- trainControl(method="repeatedcv", number=10, repeats=3)

acc.df.cv <- as.data.frame(matrix(nrow=1,ncol=1))
colnames(acc.df.cv) <- c('knn value')

for (i in 1:20) {
  knn.cv <- train(diagnosis ~., 
                    train.df,
                    tuneGrid=data.frame(k=i), 
                    method = "knn", 
                    trControl=control)
  pred.knn.cv <- predict(knn.cv, test.df)
  knn.conf.mat.cv <- confusionMatrix(pred.knn.cv,test.labels)
  acc.df.cv$Accuracy <- knn.conf.mat.cv$overall[1] # Adding accuracy to new row
  new.cv <- rep(knn.conf.mat.cv$overall[1], ncol(acc.df.cv))                    
  acc.df.cv[nrow(acc.df.cv) + 1, ] <- new.cv
}

# Cleaning up the dataframe
colnames(acc.df.cv) <- c('Accuracy', 'KNN Value')
acc.df.cv <- acc.df.cv[-c(1),]
acc.df.cv$`KNN Value` <- c(1:20)
acc.df.cv<- acc.df.cv[order(-acc.df.cv$Accuracy),] # Sorting in descending order
head(acc.df.cv)                                    # to see best value for model

# k = 11 gave the best accuracy
knn.cv.best <- train(diagnosis ~., 
                    train.df,
                    tuneGrid=data.frame(k=11),
                    method = "knn", 
                    trControl=control)
pred.knn.cv <- predict(knn.cv.best, test.df)
confmat.knn.cv <- confusionMatrix(pred.knn.cv, test.labels)
confmat.knn.cv
# This produced a slightly lower classification rate, but was cross-validated to ensure repeatability

```



```{r}
library(kernlab)
# SVM model

# Prepare the training scheme with repeated cross-validation method
control <- trainControl(method="repeatedcv", number=10, repeats=3)
svm.linear <- train(diagnosis ~., train.df, method = "svmLinear", trControl=control, tuneLength = 10)
svm.linear

pred.svm1 <- predict(svm.linear, test.df)
confmat.svm1 <- confusionMatrix(pred.svm1, test.labels)
confmat.svm1

# Using grid search to tune the SVM classifier with different cost values
grid <- expand.grid(C = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,3))
svm.linear.grid <- train(diagnosis ~., train.df, method = "svmLinear", trControl=control, tuneGrid = grid, tuneLength = 10)
svm.linear.grid
plot(svm.linear.grid)

pred.svm2 <- predict(svm.linear.grid, test.df)
confmat.svm2 <- confusionMatrix(pred.svm2, test.labels)
confmat.svm2

# Repeating with reduced features
# Prepare the training scheme with repeated cross-validation method
svm.linear.simp <- train(diagnosis ~ area_worst + texture_worst + concave.points_worst + concave.points_mean + smoothness_worst,
                    train.df, 
                    method = "svmLinear", 
                    trControl=control, 
                    tuneLength = 10)
svm.linear.simp

pred.svm2 <- predict(svm.linear.simp, test.df)
confmat.svm3 <- confusionMatrix(pred.svm2, test.labels)
confmat.svm3
# # Using grid search to tune the SVM classifier with different cost values
# grid <- expand.grid(C = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,3))
# svm.linear.grid <- train(diagnosis ~., train.df, method = "svmLinear", trControl=control, tuneGrid = grid, tuneLength = 10)
# svm.linear.grid
# plot(svm.linear.grid)
# 
# pred.svm2 <- predict(svm.linear.grid, test.df)
# confusionMatrix(pred.svm2, test.labels)

```

```{r}
# Regularization: Ridge regression

x.train = model.matrix(diagnosis~. , train.df)
y.train = train.labels
x.test = model.matrix(diagnosis~. , test.df1)
y.test = test.labels


lambda.grid <- sort(exp(seq(log(0.001), log(10^5), length.out = 1000)), decreasing = TRUE)

ridge.glm <- glmnet(x.train, 
                    y.train, 
                    alpha = 0, 
                    lambda = lambda.grid,
                    family = "binomial")
cv.ridge <- cv.glmnet(x.train, 
                      y.train, 
                      alpha = 0, 
                      lambda = lambda.grid,
                      family = "binomial")
lmin.ridge <- cv.ridge$lambda.min

print(paste("The lambda min is:",lmin.ridge))
plot(cv.ridge)

coeff.ridge <- coef(ridge.glm,lmin.ridge) 

# transform coefficient of glmnet and cvglmnet to data.frame
coeffs.dt <- data.frame(name = coeff.ridge@Dimnames[[1]][coeff.ridge@i + 1], coefficient = coeff.ridge@x)

#  reorder the variables in term of coefficients
coeffs.dt <- arrange(coeffs.dt, coefficient)
coeffs.dt

L2.ridge <- norm(coeff.ridge, type = "2")
print(paste("The L2 norm is:",L2.ridge))

# visual representation of the coefficients
ggplot(data = coeffs.dt) +
  geom_col(aes(x = name, y = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Ridge Coefficients with ", lambda, " = lmin"))) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") 

pred.ridge <- predict(ridge.glm, s = lmin.ridge, newx = x.test)
pred.ridge <- ifelse(pred.ridge > 0.5, '1', '0')
pred.ridge <- as.factor(pred.ridge) 
confmat.ridge <- confusionMatrix(pred.ridge, test.labels)
confmat.ridge

```

```{r}
# Regularization: LASSO regression

lasso.glm <- glmnet(x.train, 
                    y.train, 
                    alpha = 1, 
                    lambda = lambda.grid, 
                    family = "binomial")
cv.lasso <- cv.glmnet(x.train, 
                      y.train, 
                      alpha = 1, 
                      lambda = lambda.grid,
                      family = "binomial")
lmin.lasso <- cv.lasso$lambda.min

print(paste("The lambda min is:",lmin.lasso))
plot(cv.lasso)

L1.lasso <- norm(coeff.ridge, type = "1")
print(paste("The L1 norm is:",L1.lasso))

coeff.lasso <- coef(cv.lasso,lmin.lasso) 

# transform coefficient of glmnet and cvglmnet to data.frame
coeffs.dt2 <- data.frame(name = coeff.lasso@Dimnames[[1]][coeff.lasso@i + 1], coefficient = coeff.lasso@x)

#  reorder the variables in term of coefficients
coeffs.dt2 <- arrange(coeffs.dt2, coefficient)
coeffs.dt2

# visual representation of the coefficients
ggplot(data = coeffs.dt2) +
  geom_col(aes(x = name, y = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("LASSO Coefficients with ", lambda, " = lmin"))) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") 

pred.lasso <- plogis(predict(lasso.glm, s = lmin.lasso, newx = x.test))
pred.lasso <- ifelse(pred.lasso > 0.5, '1', '0')
pred.lasso <- as.factor(pred.lasso) 
confmat.lasso <- confusionMatrix(pred.lasso, test.labels)
confmat.lasso

``` 
```{r}
library(gamclass)
# GAM
# Running GAM using logit odds by using binomial family and restricted maximum likelihood (REML) method

gam.mod <- gam(diagnosis ~ s(area_worst) +  s(texture_worst) + s(concave.points_worst),
               data = train.df,
               family = binomial,
               method = "REML")
summary(gam.mod)

# Calculate predictions, then convert to log odds
gam.pred <- plogis(predict(gam.mod, test.df, type = "link"))

# Using 0.5 as cutoff for classifier and sorting each sample for conf. table
gam.pred <- ifelse(gam.pred > 0.5, '1', '0')
gam.pred <- as.factor(gam.pred)
confmat.gam <- confusionMatrix(gam.pred, test.labels)
confmat.gam

# Plotting partial effects of the GAM model on the probability scale, center on the intercept with intercept-related uncertainty
plot(gam.mod, pages = 1, trans = plogis,
     shift = coef(gam.mod)[1], seWithMean = TRUE)
title(main = "Visualization of Partial Effects of GAM Model")

```

```{r}
# GAM visualization using mgcViz library
library(mgcViz)

# Convert the fitted object to the gamViz class for use with mgcViz
gam.mod2 <- getViz(gam.mod)
# Calls print.plotGam()
print(plot(gam.mod2), 
      pages = 1,
      top=textGrob("Visualization of Smoothing Effects of GAM Model")) 

```

