---
title: "breast_cancer"
author: "Patrick Garr"
date: '2022-04-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(corrplot)
library(car)
library(lmtest)
library(MLmetrics)
library(glmnet)
library(caTools)
library(splines)
library(psych)
library(mgcv)
library(graphics)
library(relaimpo) # Package for calculating relative importance metrics
library(gridExtra)
library(factoextra)

set.seed(1000)
```

Dataset description (From Diagnostic Wisconsin Breast Cancer Database):

1) ID number
2) Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

a) radius (mean of distances from center to points on the perimeter)
b) texture (standard deviation of gray-scale values)
c) perimeter
d) area
e) smoothness (local variation in radius lengths)
f) compactness (perimeter^2 / area - 1.0)
g) concavity (severity of concave portions of the contour)
h) concave points (number of concave portions of the contour)
i) symmetry
j) fractal dimension ("coastline approximation" - 1)

The mean, standard error and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

All feature values are recoded with four significant digits.


```{r}
# Wisconsin diagnostic breast cancer dataset
wdbc.data <- read.csv("wdbc.csv", sep = ",",
         header = TRUE, stringsAsFactors = TRUE)

# Removing the id attribute as it's not needed for this analysis. Also removing unknown X variable with no data in it
wdbc.data <- wdbc.data[,-1]
wdbc.data <- wdbc.data[,-32]

nrows.LE <- nrow(wdbc.data)
ncol.LE <- length(wdbc.data)

print(paste("There are",nrows.LE, "rows and",ncol.LE, "columns in the breast cancer dataset"))

summary(wdbc.data)

```



```{r}
# Creating a box plot examining the ten factors' means with diagnosis outcomes to get a simple graphical view
boxplot1 <- ggplot(wdbc.data, aes(radius_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='radius vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot2 <- ggplot(wdbc.data, aes(texture_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='texture vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot3 <- ggplot(wdbc.data, aes(perimeter_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='perimeter vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot4 <- ggplot(wdbc.data, aes(area_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='area vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot5 <- ggplot(wdbc.data, aes(smoothness_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='smoothness vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot6 <- ggplot(wdbc.data, aes(compactness_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='compactness vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot7 <- ggplot(wdbc.data, aes(concavity_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='concavity vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot8 <- ggplot(wdbc.data, aes(concave.points_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='concave points vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot9 <- ggplot(wdbc.data, aes(symmetry_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='symmetry vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot10 <- ggplot(wdbc.data, aes(fractal_dimension_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='fractal dimension vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

grid.arrange(boxplot1,boxplot2,boxplot3,boxplot4,boxplot5,boxplot6,boxplot7,boxplot8,boxplot9,boxplot10, ncol=4, nrow =3)

```


```{r}

# corrplot(cor(wdbc.data[,2:31]), method = 'circle', title = "Correlation Plot of Breast Cancer Dataset Variables", mar=c(0,0,1,0))

```


```{r}
# Changing the benign and malignant diagnoses to 1 and 0, respectively
wdbc.data$diagnosis <- as.character(wdbc.data$diagnosis)
wdbc.data$diagnosis <- replace(wdbc.data$diagnosis, wdbc.data$diagnosis == "B","1")
wdbc.data$diagnosis <- replace(wdbc.data$diagnosis, wdbc.data$diagnosis == "M","0")
wdbc.data$diagnosis <- as.factor(wdbc.data$diagnosis)

# Scaling and indexing data to create training and testing sets. Using 80% and 20% because dataset is a little on the small side
wdbc.df <- wdbc.data
wdbc.df[2:31] <- scale(wdbc.df[2:31])

idx = sample(nrow(wdbc.df),nrow(wdbc.df)*0.8)

train.df = as.data.frame((wdbc.df[idx,]))
test.df = as.data.frame(wdbc.df[-idx,])

train.labels <- train.df[,1]
test.labels <- test.df[,1]

```

```{r}
# Performing PCA to see if we can narrow down the features
pr.wdbc <- prcomp(wdbc.df[2:31], scale=FALSE)
get_eig(pr.wdbc)
# 3 PCs explain 73% of the data, 5 PCs explain 85%, and 10 PCs explain 95%

# Looking at the top variables in the top 3 PCs
wdbc.load <- pr.wdbc$rotation
PC1.top10 <- sort(abs(wdbc.load[,1]),decreasing = TRUE)[1:10]
PC2.top10 <- sort(abs(wdbc.load[,2]),decreasing = TRUE)[1:10]
PC3.top10 <- sort(abs(wdbc.load[,3]),decreasing = TRUE)[1:10]

fviz_eig(pr.wdbc, addlabels = TRUE)
# Significant elbow seen after the 2nd or 3rd PC

fviz_pca_biplot(pr.wdbc, repel = TRUE)

```
```{r}
# Hierarchical clustering: Euclidian method
dist.wdbc1 = dist(wdbc.df[2:31], method = "euclidean")
fviz_dist(dist.wdbc1)+
  labs(title = "Euclidian Dissimilarity Matrix")

hcl.wdbc1 <- hclust(dist.wdbc1, method = 'complete')
plot(hcl.wdbc1, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc1, k = 2, border = 'red')

cutHcl1 = cutree(hcl.wdbc1, k = 2)
clusterTab1 = table(cutHcl1, wdbc.df[,1])
clusterTab1

```
```{r}
# Hierarchical clustering: Euclidian method w/ larger k

# Finding optimal number of ks using a for loop, commenting it after running
# range <- c(1:10)
# 
# for (val in range) { 
#   cutHcl = cutree(hcl.wdbc, k =val)
#   clusterTab = table(cutHcl, wdbc.df[,1])
#   print(clusterTab)
#   print(paste0(val, " Clusters"))}

# Optimal clusters seems to be at 4 as that is where the largest difference in clusters occurs

dist.wdbc2 = dist(wdbc.df[2:31], method = "euclidean")

hcl.wdbc2 <- hclust(dist.wdbc2, method = 'complete')
plot(hcl.wdbc2, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc2, k = 4, border = 'red')

cutHcl2 = cutree(hcl.wdbc2, k = 4)
clusterTab2 = table(cutHcl2, wdbc.df[,1])
clusterTab2

```

```{r}
# Hierarchical clustering: Manhattan method

dist.wdbc3 = dist(wdbc.df[2:31], method = "manhattan")

hcl.wdbc3 <- hclust(dist.wdbc3, method = 'ward.D')
plot(hcl.wdbc3, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc3, k = 2, border = 'red')

cutHcl3 = cutree(hcl.wdbc3, k = 2)
clusterTab3 = table(cutHcl3, wdbc.df[,1])
clusterTab3

```


```{r}

lda.bc = lda(diagnosis ~., data = train.df)

pred.lda1 = predict(lda.bc, train.df)
pred.lda2 = predict(lda.bc, test.df)

confmat.lda1 <- confusionMatrix(pred.lda1$class, train.labels)
confmat.lda2 <- confusionMatrix(pred.lda2$class, test.labels)

confmat.lda1
confmat.lda2

# print(paste0("The misclassification rate of the training set is: ", (1-confmat.lda1$overall[1])*100, "% and the rate of the test set is: ", (1-confmat.lda2$overall[1])*100,"%"))
# print(paste0("The precision of the training set is: ", confmat.lda1$byClass[5]*100, "% and the that of the test set is: ", confmat.lda2$byClass[5]*100,"%"))

```


```{r}
# Logistic regression

wdbc.log = glm(diagnosis ~., family = binomial, data = train.df)
log.fit.cont = predict(wdbc.log, newdata = test.df, type = 'response')
log.fit = ifelse(log.fit.cont > 0.5, '1', '0')

log.fit.train = predict(wdbc.log, newdata = train.df, type = 'response')
log.fit.train = ifelse(log.fit.train > 0.5, '1', '0')

pred.log.train = as.factor(log.fit.train)
pred.log.test = as.factor(log.fit)

confmat.log1 <- confusionMatrix(pred.log.train, train.df[,1])
confmat.log2 <- confusionMatrix(pred.log.test, test.df[,1])

confmat.log1
confmat.log2

```


```{r}
# Write a function to calculcate the misclassification rate
cal_misclassification <- function(confusionTab){
  misc = (confusionTab[1,2]+confusionTab[2,1])/sum(confusionTab)
  print(paste("misclassification rate: ", misc))
}

```



```{r}
# Use a decision tree to find which features are more significant
wdbc.tree = tree(diagnosis~., data =train.df)
summary(pima.tree)


# Make a plot of the tree to visulize
plot(wdbc.tree)
text(wdbc.tree, pretty = 0)

# One of the problems, however, is that they tend to overfit when used 'out of the box'
treePred = predict(wdbc.tree, test.df, type='class')
confusionTab = table(Predicted = treePred, Actual = test.df$diagnosis)
confusionTab
cal_misclassification(confusionTab)

```
According to the confusion matrix, we could see that the misclassification rate is very low.


```{r}
# Although the model already has very good performance, we Use cross validation to see if the performance could be further improved, or if we could reach similar performance use a more simple model.
cv.wdbc.tree = cv.tree(wdbc.tree, FUN = prune.misclass)
plot(cv.wdbc.tree)


prune.wdbc = prune.misclass(wdbc.tree, best = 6)
plot(prune.wdbc)
text(prune.wdbc, pretty = 0)

treePred2 = predict(prune.wdbc, test.df, type='class')
confusionTab2 = table(Predicted = treePred2, Actual = test.df$diagnosis)
confusionTab2

cal_misclassification(confusionTab2)
```
We could see that the model has smaller size with same misclassification rate now.
