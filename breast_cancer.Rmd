---
title: "breast_cancer"
author: "Patrick Garr"
date: '2022-04-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(corrplot)
library(car)
library(lmtest)
library(MLmetrics)
library(glmnet)
library(caTools)
library(splines)
library(psych)
library(mgcv)
library(graphics)
library(relaimpo) # Package for calculating relative importance metrics
library(gridExtra)
library(factoextra)
library(ROCR)
library(MASS)
library(gmodels)
library(class)
library(e1071) # Used for SVM model
library(mlbench) # Used for LVQ model

set.seed(1000)
```

```{r}
# Wisconsin diagnostic breast cancer dataset
wdbc.data <- read.csv("wdbc.csv", sep = ",",
         header = TRUE, stringsAsFactors = TRUE)

# Removing the id attribute as it's not needed for this analysis. Also removing unknown X variable with no data in it
wdbc.data <- wdbc.data[,-1]
wdbc.data <- wdbc.data[,-32]

nrows.LE <- nrow(wdbc.data)
ncol.LE <- length(wdbc.data)

print(paste("There are",nrows.LE, "rows and",ncol.LE, "columns in the breast cancer dataset"))

summary(wdbc.data)

```


```{r}
# Creating a box plot examining the ten factors' means with diagnosis outcomes to get a simple graphical view
boxplot1 <- ggplot(wdbc.data, aes(radius_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='radius vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot2 <- ggplot(wdbc.data, aes(texture_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='texture vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot3 <- ggplot(wdbc.data, aes(perimeter_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='perimeter vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot4 <- ggplot(wdbc.data, aes(area_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='area vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot5 <- ggplot(wdbc.data, aes(smoothness_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='smoothness vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot6 <- ggplot(wdbc.data, aes(compactness_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='compactness vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot7 <- ggplot(wdbc.data, aes(concavity_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='concavity vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot8 <- ggplot(wdbc.data, aes(concave.points_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='concave points vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot9 <- ggplot(wdbc.data, aes(symmetry_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='symmetry vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

boxplot10 <- ggplot(wdbc.data, aes(fractal_dimension_mean, diagnosis))+
  geom_boxplot(outlier.colour="red")+
  coord_flip()+
  labs(title='fractal dimension vs. diagnosis ')+
  theme(plot.title = element_text(hjust = 0.5, size = 6))

grid.arrange(boxplot1,boxplot2,boxplot3,boxplot4,boxplot5,boxplot6,boxplot7,boxplot8,boxplot9,boxplot10, ncol=4, nrow =3)

```

```{r}
# Creating a density plot examining the ten factors' means
pt1 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$radius_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = radius_mean, y=..density..)) +
    labs(title='density function of radius_mean', x='radius_mean', y='density') 
pt2 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$texture_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = texture_mean, y=..density..)) +
    labs(title='density function of texture_mean', x='texture_mean', y='density')
pt3 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$perimeter_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = perimeter_mean, y=..density..)) +
    labs(title='density function of perimeter_mean', x='perimeter_mean', y='density')
pt4 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$area_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = area_mean, y=..density..)) +
    labs(title='density function of area_mean', x='area_mean', y='density')
pt5 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$smoothness_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = smoothness_mean, y=..density..)) +
    labs(title='density function of smoothness_mean', x='smoothness_mean', y='density')
pt6 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$compactness_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = compactness_mean, y=..density..)) +
    labs(title='density function of compactness_mean', x='compactness_mean', y='density')
pt7 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$concavity_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = concavity_mean, y=..density..)) +
    labs(title='density function of concavity_mean', x='concavity_mean', y='density')
pt8 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$concave.points_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = concave.points_mean, y=..density..)) +
    labs(title='density function of concave.points_mean', x='concave.points_mean', y='density')
pt9 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$symmetry_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = symmetry_mean, y=..density..)) +
    labs(title='density function of symmetry_mean', x='symmetry_mean', y='density')
pt10 = ggplot(data = wdbc.data) +
    geom_histogram(aes(x = df$fractal_dimension_mean, y=..density..), colour = "black", fill = "red") + geom_density(aes(x = fractal_dimension_mean, y=..density..)) +
    labs(title='density function of fractal_dimension_mean', x='fractal_dimension_mean', y='density')
# grid.arrange(pt1,pt2,pt3,pt4,pt5,pt6,pt7,pt8,pt9,pt10, ncol=4, nrow =3)
```


```{r}

corrplot(cor(wdbc.data[,2:31]), method = 'circle', title = "Correlation Plot of Breast Cancer Dataset Variables", mar=c(0,0,1,0))
# wdbc.matrix <- as.matrix(wdbc.data[,2:31])
# heatmap(wdbc.matrix)

```



```{r}
# Changing the benign and malignant diagnoses to 1 and 0, respectively
wdbc.data$diagnosis <- as.character(wdbc.data$diagnosis)
wdbc.data$diagnosis <- replace(wdbc.data$diagnosis, wdbc.data$diagnosis == "B","1")
wdbc.data$diagnosis <- replace(wdbc.data$diagnosis, wdbc.data$diagnosis == "M","0")
wdbc.data$diagnosis <- as.factor(wdbc.data$diagnosis)

# Scaling and indexing data to create training and testing sets. Using 70% split
wdbc.df <- wdbc.data
wdbc.df[2:31] <- scale(wdbc.df[2:31])

idx <- sample(nrow(wdbc.df),nrow(wdbc.df)*0.7)

train.df <- as.data.frame((wdbc.df[idx,]))
test.df1 <- as.data.frame(wdbc.df[-idx,])
test.df <- test.df1[,-c(1)]

train.labels <- train.df[,1]
test.labels <- test.df1[,1]

```

```{r}
# Feature selection with R caret using a Learning Vector Quantization model
# Prepare the training scheme with repeated cross-validation method
control <- trainControl(method="repeatedcv", number=10, repeats=3)
lvq.wdbc <- train(diagnosis~., data=wdbc.data, method="lvq", preProcess="scale", trControl=control)

# Use the model to estimate relative importances
importance <- varImp(lvq.wdbc, scale=FALSE)
print(importance)
plot(importance)

```

```{r}
# Performing PCA to see if we can narrow down the features
pr.wdbc <- prcomp(wdbc.df[2:31], scale=FALSE)
get_eig(pr.wdbc)
# 3 PCs explain 73% of the data, 5 PCs explain 85%, and 10 PCs explain 95%

# Looking at the top variables in the top 3 PCs
wdbc.load <- pr.wdbc$rotation
PC1.top10 <- sort(abs(wdbc.load[,1]),decreasing = TRUE)[1:10]
PC2.top10 <- sort(abs(wdbc.load[,2]),decreasing = TRUE)[1:10]
PC3.top10 <- sort(abs(wdbc.load[,3]),decreasing = TRUE)[1:10]

fviz_eig(pr.wdbc, addlabels = TRUE)
# Significant elbow seen after the 2nd or 3rd PC

fviz_pca_biplot(pr.wdbc, repel = TRUE)

```

```{r}
# k-means clustering

# Determining the optimal number of clusters using two methods
fviz_nbclust(wdbc.df[2:31], kmeans, method ="wss") +
  labs(subtitle = "Wss Method")
fviz_nbclust(wdbc.df[2:31], kmeans, method ="silhouette")+
  labs(subtitle = "Silhouette Method")
# Both methods indicate that 2 clusters is enough 

kmeans.wdbc = kmeans(wdbc.df[2:31], center = 2)
# kmeans_df3

fviz_cluster(kmeans.wdbc, wdbc.df[2:31])+
  labs(subtitle = "k-means Model of Breast Cancer Data with 2 Clusters")

table(kmeans.wdbc$cluster,wdbc.df$diagnosis)

```


```{r}
# Hierarchical clustering: Euclidian method
dist.wdbc1 = dist(wdbc.df[2:31], method = "euclidean")
fviz_dist(dist.wdbc1)+
  labs(title = "Euclidian Dissimilarity Matrix")

hcl.wdbc1 <- hclust(dist.wdbc1, method = 'complete')
plot(hcl.wdbc1, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc1, k = 2, border = 'red')

cutHcl1 = cutree(hcl.wdbc1, k = 2)
clusterTab1 = table(cutHcl1, wdbc.df[,1])
clusterTab1

```


```{r}
# Hierarchical clustering: Euclidian method w/ larger k

# Finding optimal number of ks using a for loop, commenting it after running
# range <- c(1:10)
# 
# for (val in range) { 
#   cutHcl = cutree(hcl.wdbc, k =val)
#   clusterTab = table(cutHcl, wdbc.df[,1])
#   print(clusterTab)
#   print(paste0(val, " Clusters"))}

# Optimal clusters seems to be at 4 as that is where the largest difference in clusters occurs

dist.wdbc2 = dist(wdbc.df[2:31], method = "euclidean")

hcl.wdbc2 <- hclust(dist.wdbc2, method = 'complete')
plot(hcl.wdbc2, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc2, k = 4, border = 'red')

cutHcl2 = cutree(hcl.wdbc2, k = 4)
clusterTab2 = table(cutHcl2, wdbc.df[,1])
clusterTab2

```

```{r}
# Hierarchical clustering: Manhattan method

dist.wdbc3 = dist(wdbc.df[2:31], method = "manhattan")

hcl.wdbc3 <- hclust(dist.wdbc3, method = 'ward.D')
plot(hcl.wdbc3, cex = 0.6, hang = -1)
rect.hclust(hcl.wdbc3, k = 2, border = 'red')

cutHcl3 = cutree(hcl.wdbc3, k = 2)
clusterTab3 = table(cutHcl3, wdbc.df[,1])
clusterTab3

```
```{r}
# Write a function to calculcate the misclassification rate
cal_misclassification <- function(confusionTab){
  misc = (confusionTab[1,2]+confusionTab[2,1])/sum(confusionTab)
  print(paste("misclassification rate: ", misc))
}
```



```{r}
# use unscaled data
train.tr <- as.data.frame((wdbc.data[idx,]))
test.tr1 <- as.data.frame(wdbc.data[-idx,])
test.tr <- test.tr1[,-c(1)]
train.labels <- train.df[,1]
test.labels.tr <- test.tr1[,1]

library(tree)
# Use a decision tree to find which features are more significant
wdbc.tree = tree(diagnosis~., data =train.tr)
summary(wdbc.tree)
# Make a plot of the tree to visulize
plot(wdbc.tree)
text(wdbc.tree, pretty = 0)
# One of the problems, however, is that they tend to overfit when used 'out of the box'
treePred = predict(wdbc.tree, test.tr, type='class')
confusionTab = table(Predicted = treePred, Actual = test.labels.tr)
confusionTab
cal_misclassification(confusionTab)

confusionMatrix(treePred,test.labels.tr)
```
According to the confusion matrix, we could see that the misclassification rate is very low.

```{r}

# mtry is the number of variables randomly sampled at each split. Use sqrt(p) here for classification
library(randomForest)
wdbc.rf = randomForest(diagnosis~., data = train.tr, mtry = sqrt(ncol(train.tr)), ntree = 500)
wdbc.rf

rfPreds = predict(wdbc.rf, test.tr)


plot(wdbc.rf)
summary(wdbc.rf)
importance(wdbc.rf)
varImpPlot(wdbc.rf)


bestmtry <- tuneRF(train.tr[2:31], train.tr$diagnosis, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)

# Use the best mtry
optRf = randomForest(diagnosis~., data = train.tr, mtry = 7, ntree = 320)
plot(optRf)
importance(optRf)
varImpPlot(optRf)

```


```{r}
# Although the model already has very good performance, we Use cross validation to see if the performance could be further improved, or if we could reach similar performance use a more simple model.
cv.wdbc.tree = cv.tree(wdbc.tree, FUN = prune.misclass)
plot(cv.wdbc.tree)
prune.wdbc = prune.misclass(wdbc.tree, best = 6)
plot(prune.wdbc)
text(prune.wdbc, pretty = 0)
treePred2 = predict(prune.wdbc, test.df, type='class')
confusionTab2 = table(Predicted = treePred2, Actual = test.labels)
confusionTab2
cal_misclassification(confusionTab2)
```
We could see that the model has smaller size with same misclassification rate now.



```{r}
# LDA

lda.bc = lda(diagnosis ~., data = train.df)

pred.lda1 = predict(lda.bc, train.df)
pred.lda2 = predict(lda.bc, test.df)

confmat.lda1 <- confusionMatrix(pred.lda1$class, train.labels)
confmat.lda2 <- confusionMatrix(pred.lda2$class, test.labels)

confmat.lda1
confmat.lda2

# Making ROC curves for better visualization of the data
ROC.df.lda <- data.frame(pred.lda2$posterior[,2])
ROC.df.lda$labels <- test.labels

ROC.pred.lda <- prediction(ROC.df.lda[1],ROC.df.lda[2])
ROC.perf.lda <- performance(ROC.pred.lda, "tpr","fpr")
plot(ROC.perf.lda, main = "ROC Curve of LDA Model")

auc.lda <- performance(ROC.pred.lda, measure = 'auc')
auc.lda <- auc.lda@y.values
print(paste("The AUC of this curve is:", auc.lda))

```


```{r}
# Logistic regression

wdbc.log = glm(diagnosis ~., family = binomial, data = train.df)
log.fit.cont = predict(wdbc.log, newdata = test.df, type = 'response')
log.fit = ifelse(log.fit.cont > 0.5, '1', '0')

log.fit.train = predict(wdbc.log, newdata = train.df, type = 'response')
log.fit.train = ifelse(log.fit.train > 0.5, '1', '0')

pred.log.train = as.factor(log.fit.train)
pred.log.test = as.factor(log.fit)

confmat.log1 <- confusionMatrix(pred.log.train, train.labels)
confmat.log2 <- confusionMatrix(pred.log.test, test.labels)

confmat.log1
confmat.log2

# Making ROC curves for better visualization of the data
ROC.log <- data.frame(log.fit.cont)
ROC.log$labels <- test.labels

ROC.pred.log <- prediction(ROC.log[1],ROC.log[2])
ROC.perf.log <- performance(ROC.pred.log, "tpr","fpr")
plot(ROC.perf.log, main = "ROC Curve of Logistic Regression Model")

auc.log <- performance(ROC.pred.log, measure = 'auc')
auc.log <- auc.log@y.values
print(paste("The AUC of this curve is:", auc.log))

```

```{r}
# knn model

# Determining best k using a for loop to create multiple models and compare their accuracy values
# Initialize the dataframe
acc.df <- as.data.frame(matrix(nrow=1,ncol=1))
colnames(acc.df) <- c('knn value')

for (i in 1:20) {
  knn.wdbc <- knn(train.df,test.df1,train.labels, k = i)
  knn.conf.mat <- confusionMatrix(knn.wdbc,test.labels)
  acc.df$Accuracy <- knn.conf.mat$overall[1] # Adding the accuracy to a new row
  new <- rep(knn.conf.mat$overall[1], ncol(acc.df))                       
  acc.df[nrow(acc.df) + 1, ] <- new
}

# Cleaning up the dataframe
colnames(acc.df) <- c('Accuracy', 'KNN Value')
acc.df <- acc.df[-c(1),]
acc.df$`KNN Value` <- c(1:20)
acc.df<- acc.df[order(-acc.df$Accuracy),] # Sorting in descending order to see
head(acc.df)                              # best value for model

# Multiple values give the same accuracy so choosing 10 for the model to keep it flexible for other datasets if needed

knn.wdbc.best <- knn(train.df,test.df1,train.labels, k = 10)
confusionMatrix(knn.wdbc.best,test.labels)

```



```{r}
library(kernlab)
# SVM model

# Prepare the training scheme with repeated cross-validation method
control <- trainControl(method="repeatedcv", number=10, repeats=3)
svm.linear <- train(diagnosis ~., train.df, method = "svmLinear", trControl=control, tuneLength = 10)
svm.linear

pred.svm1 <- predict(svm.linear, test.df)
confusionMatrix(pred.svm1, test.labels)

# Using grid search to tune the SVM classifier with different cost values
grid <- expand.grid(C = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,3))
svm.linear.grid <- train(diagnosis ~., train.df, method = "svmLinear", trControl=control, tuneGrid = grid, tuneLength = 10)
svm.linear.grid
plot(svm.linear.grid)

pred.svm2 <- predict(svm.linear.grid, test.df)
confusionMatrix(pred.svm2, test.labels)

```

```{r}
# Regularization: Ridge regression

reg.data <- wdbc.df

# Changing the malignant diagnoses -1 for use in regularization
reg.data$diagnosis <- as.character(reg.data$diagnosis)
reg.data$diagnosis <- replace(reg.data$diagnosis, reg.data$diagnosis == "0","-1")
reg.data$diagnosis <- as.numeric(reg.data$diagnosis)

# Creating test and train sets for these regressions
idx.reg <- sample(nrow(reg.data),nrow(reg.data)*0.7)

train.df.reg <- as.data.frame((reg.data[idx.reg,]))
test.df.reg <- as.data.frame(reg.data[-idx.reg,])

train.labels.reg <- train.df.reg[,1]
test.labels.reg <- test.df.reg[,1]
# Creating test labels as factor for later use in prediction tables
test.labels.reg.fact <- as.factor(test.labels.reg) 

x.train = model.matrix(diagnosis~. , train.df.reg)
y.train = train.labels.reg
x.test = model.matrix(diagnosis~. , test.df.reg)
y.test = test.labels.reg


lambda.grid <- sort(exp(seq(log(0.001), log(10^5), length.out = 1000)), decreasing = TRUE)

ridge.glm <- glmnet(x.train, y.train, alpha = 0, lambda = lambda.grid)
cv.ridge <- cv.glmnet(x.train, y.train, alpha = 0, lambda = lambda.grid)
lmin.ridge <- cv.ridge$lambda.min

print(paste("The lambda min is:",lmin.ridge))
plot(cv.ridge)

coeff.ridge <- coef(ridge.glm,lmin.ridge) 

# transform coefficient of glmnet and cvglmnet to data.frame
coeffs.dt <- data.frame(name = coeff.ridge@Dimnames[[1]][coeff.ridge@i + 1], coefficient = coeff.ridge@x)

#  reorder the variables in term of coefficients
coeffs.dt <- arrange(coeffs.dt, coefficient)
coeffs.dt

L2.ridge <- norm(coeff.ridge, type = "2")
print(paste("The L2 norm is:",L2.ridge))

# visual representation of the coefficients
ggplot(data = coeffs.dt) +
  geom_col(aes(x = name, y = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Ridge Coefficients with ", lambda, " = lmin"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") 

pred.ridge <- predict(ridge.glm, s = lmin.ridge, newx = x.test)
pred.ridge <- ifelse(pred.ridge > 0, '1', '-1')
pred.ridge <- as.factor(pred.ridge) 
confusionMatrix(pred.ridge, test.labels.reg.fact)

```

```{r}
# Regularization: LASSO regression

lasso.glm <- glmnet(x.train, y.train, alpha = 1, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x.train, y.train, alpha = 1, lambda = lambda.grid)
lmin.lasso <- cv.lasso$lambda.min

print(paste("The lambda min is:",lmin.lasso))
plot(cv.lasso)

L1.lasso <- norm(coeff.ridge, type = "1")
print(paste("The L1 norm is:",L1.lasso))

coeff.lasso <- coef(cv.lasso,lmin.lasso) 

# transform coefficient of glmnet and cvglmnet to data.frame
coeffs.dt2 <- data.frame(name = coeff.lasso@Dimnames[[1]][coeff.lasso@i + 1], coefficient = coeff.lasso@x)

#  reorder the variables in term of coefficients
coeffs.dt2 <- arrange(coeffs.dt2, coefficient)
coeffs.dt2

# visual representation of the coefficients
ggplot(data = coeffs.dt2) +
  geom_col(aes(x = name, y = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients with ", lambda, " = lmin"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") 

pred.lasso <- predict(lasso.glm, s = lmin.lasso, newx = x.test)
pred.lasso <- ifelse(pred.lasso > 0, '1', '-1')
pred.lasso <- as.factor(pred.lasso) 
confusionMatrix(pred.lasso, test.labels.reg.fact)

``` 

```{r}
# GAM
gam.mod <- gam(diagnosis ~ s(radius_worst) + s(area_worst) + s(perimeter_worst) + s(concave.points_mean) + s(concave.points_worst), data = train.df.reg)
summary(gam.mod)

gam.pred <- predict(gam.mod,test.df.reg)
gam.pred <- ifelse(gam.pred > 0, '1', '-1')
gam.pred <- as.factor(gam.pred) 
confusionMatrix(gam.pred, test.labels.reg.fact)

```

